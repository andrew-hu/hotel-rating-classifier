{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "input_file = \"Hotel_Reviews.csv\"\n",
    "\n",
    "# Load Hotel_reviews data\n",
    "df = pd.read_csv(input_file, header = 0)\n",
    "data_pandas = pd.DataFrame(df)\n",
    "\n",
    "df['is_train'] = np.random.uniform(0,1,len(df)) <= 0.75\n",
    "\n",
    "score_bucket = []\n",
    "i=0\n",
    "for row in data_pandas.iloc[:,12]:\n",
    "    if row > 8:\n",
    "        score_bucket.append(5)\n",
    "    elif row > 6:\n",
    "        score_bucket.append(4)\n",
    "    elif row > 4:\n",
    "        score_bucket.append(3)\n",
    "    elif row > 2:\n",
    "        score_bucket.append(2)\n",
    "    elif row > 0:\n",
    "        score_bucket.append(1)\n",
    "    i = i + 1\n",
    "\n",
    "df['processed_score'] = score_bucket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use\n",
      "booking\n",
      "hotel\n",
      "small\n",
      "room\n",
      "floor\n",
      "booked\n",
      "window\n",
      "asked\n",
      "double\n",
      "day\n",
      "check\n",
      "time\n",
      "view\n",
      "noise\n",
      "negative\n",
      "rooms\n",
      "nice\n",
      "bit\n",
      "tea\n",
      "coffee\n",
      "bar\n",
      "door\n",
      "shower\n",
      "staff\n",
      "clean\n",
      "bed\n",
      "water\n",
      "noisy\n",
      "old\n",
      "night\n",
      "told\n",
      "price\n",
      "good\n",
      "great\n",
      "work\n",
      "breakfast\n",
      "restaurant\n",
      "pay\n",
      "bathroom\n",
      "facilities\n",
      "hot\n",
      "bad\n",
      "morning\n",
      "open\n",
      "parking\n",
      "need\n",
      "little\n",
      "people\n",
      "stay\n",
      "reception\n",
      "area\n",
      "expensive\n",
      "cold\n",
      "quite\n",
      "poor\n",
      "better\n",
      "service\n",
      "food\n",
      "location\n",
      "wifi\n",
      "air\n",
      "far\n",
      "beautiful\n",
      "excellent\n",
      "fantastic\n",
      "lovely\n",
      "amazing\n",
      "modern\n",
      "place\n",
      "spacious\n",
      "quiet\n",
      "friendly\n",
      "positive\n",
      "big\n",
      "walk\n",
      "walking\n",
      "distance\n",
      "comfy\n",
      "comfortable\n",
      "large\n",
      "close\n",
      "city\n",
      "super\n",
      "easy\n",
      "loved\n",
      "helpful\n",
      "extremely\n",
      "definitely\n",
      "restaurants\n",
      "metro\n",
      "station\n",
      "central\n",
      "perfect\n",
      "wonderful\n",
      "beds\n",
      "best\n",
      "free\n",
      "value\n",
      "size\n",
      "convenient\n",
      "london\n",
      "tube\n"
     ]
    }
   ],
   "source": [
    "mylist = list(df)\n",
    "del mylist[0:20]\n",
    "mylist\n",
    "\n",
    "for word in mylist:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_bucket=[]\n",
    "\n",
    "              # Prepositions\n",
    "neutral = set([\"with\",\"at\", \"from\", \"into\", \"during\", \"including\", \"until\", \"against\", \"among\", \"throughout\", \"despite\",\n",
    "              \"towards\", \"upon\", \"of\", \"to\", \"in\", \"for\", \"on\", \"by\", \"about\", \"like\", \"through\", \"over\", \"before\", \"between\",\n",
    "              \"after\", \"since\", \"without\", \"under\", \"within\", \"along\", \"following\", \"across\", \"behind\", \"beyond\", \"plus\", \"except\",\n",
    "              \"but\", \"up\", \"out\", \"around\", \"down\", \"off\", \"above\", \"near\",\n",
    "              # Conjuctions\n",
    "              \"and\", \"or\", \"but\", \"nor\", \"so\", \"for\", \"yet\", \"after\", \"although\", \"as\", \"because\", \"before\", \"even\", \"though\",\n",
    "              \"once\", \"since\", \"till\", \"unless\", \"until\", \"what\", \"when\", \"whenever\", \"wherever\", \"whether\", \"while\",\n",
    "              # Pronouns\n",
    "              \"i\", \"you\", \"he\", \"she\", \"it\", \"we\", \"they\", \"me\", \"him\", \"her\", \"us\", \"them\", \"what\", \"who\", \"this\", \"that\",\n",
    "              \"these\", \"those\",\n",
    "               # etc\n",
    "               \"a\", \"all\" ,\"an\", \"am\", \"be\", \"can\", \"did\", \"didn\", \"get\", \"got\", \"had\", \"have\", \"if\", \"my\", \"next\", \"our\", \"some\",\n",
    "               \"the\", \"was\", \"were\", \"would\", \"your\", \"just\", \"like\", \"really\", \"wasn\"\n",
    "              ])\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#Finding most important words in Negative Reviews\n",
    "negative_comment = df['Negative_Review']\n",
    "cv = CountVectorizer(analyzer = \"word\",stop_words = 'english',max_features = 70,ngram_range=(1,1))\n",
    "most_negative_words = cv.fit_transform(negative_comment)\n",
    "temp1_counts = most_negative_words.sum(axis=0)\n",
    "temp1_words = cv.vocabulary_\n",
    "\n",
    "cv2 = CountVectorizer(analyzer = \"word\",stop_words = 'english',max_features = 70,ngram_range=(1,1))\n",
    "positive_comment = df['Positive_Review']\n",
    "most_positive_words = cv2.fit_transform(positive_comment)\n",
    "temp2_counts = most_positive_words.sum(axis=0)\n",
    "temp2_words = cv2.vocabulary_\n",
    "\n",
    "for row in data_pandas.iloc[:,6]:\n",
    "    mylist = row.split()\n",
    "    no_nums = [x for x in mylist if not (x.isdigit() or x[0] == '-' and x[1:].isdigit())]\n",
    "    \n",
    "    # Filter out caps\n",
    "    no_nums = map(str.lower, no_nums)\n",
    "    \n",
    "    row_list={}\n",
    "    temp_words = {**temp1_words, **temp2_words}\n",
    "\n",
    "    for word in temp_words:\n",
    "        if word in neutral:\n",
    "            continue\n",
    "        elif word.lower() not in no_nums:\n",
    "            row_list[word] = 0\n",
    "        else:\n",
    "            row_list[word] = 1\n",
    "       \n",
    "    list_bucket.append(row_list)        \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dict = {}\n",
    "for entry in list_bucket:\n",
    "    for key in entry:\n",
    "        dict[key] = []\n",
    "    break    \n",
    "for entry in list_bucket:\n",
    "    for key in entry:\n",
    "            dict[key].append(entry[key])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for entry in list_bucket:\n",
    "    for key in entry:\n",
    "        df[key] = dict[key]\n",
    "    break  \n",
    "#data_pandas.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from numpy import genfromtxt\n",
    "\n",
    "# df['is_train'] = np.random.uniform(0,1,len(df)) <= 0.75\n",
    "# define training and test sets\n",
    "train = df[df['is_train']==True]\n",
    "test = df[df['is_train']==False]\n",
    "\n",
    "trainTargets = np.array(train['processed_score']).astype(int)\n",
    "testTargets = np.array(test['processed_score']).astype(int)\n",
    "\n",
    "# columns you want to model\n",
    "features = df.columns[20:159]\n",
    "\n",
    "# call Gaussian Naive Bayesian class with default parameters\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# train model\n",
    "#y_gnb = gnb.fit(train[features], trainTargets).predict(train[features])\n",
    "y_gnb = gnb.fit(train[features], trainTargets).predict(test[features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65128835501338145"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(testTargets, y_gnb)\n",
    "#gnb.predict([[0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
